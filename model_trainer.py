# model_trainer.py
import os
import torch
import torch.optim as optim  # ä¼˜åŒ–å™¨
import joblib  # ä¿å­˜label_encoderç”¨çš„


class BERTTrainer:
    """
    BERTæ¨¡å‹çš„è®­ç»ƒå™¨ç±»ï¼Œå°è£…äº†è®­ç»ƒã€è¯„ä¼°ã€ä¿å­˜æ¨¡å‹çš„åŠŸèƒ½
    è¿™æ ·å†™ä»£ç çœ‹èµ·æ¥æ¸…æ¥šç‚¹ï¼Œä¸ç„¶å…¨å †åœ¨ä¸€èµ·ä¹±
    """
    def __init__(self, model, device, save_path):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨
        - model: è¦è®­ç»ƒçš„æ¨¡å‹
        - device: è®­ç»ƒç”¨çš„è®¾å¤‡ï¼ˆCPU/GPUï¼‰
        - save_path: æ¨¡å‹ä¿å­˜çš„æ ¹ç›®å½•
        """
        self.model = model  # æ¨¡å‹
        self.device = device  # è®¾å¤‡
        self.save_path = save_path  # ä¿å­˜è·¯å¾„
        # åˆ›å»ºä¿å­˜ç›®å½•ï¼Œä¸å­˜åœ¨å°±æ–°å»º
        os.makedirs(save_path, exist_ok=True)

    def train_epoch(self, train_loader, optimizer, epoch, total_epochs):
        """
        è®­ç»ƒä¸€ä¸ªepochï¼ˆæŠŠæ‰€æœ‰è®­ç»ƒæ•°æ®è·‘ä¸€éï¼‰

        å‚æ•°ï¼š
        - train_loader: è®­ç»ƒé›†åŠ è½½å™¨
        - optimizer: ä¼˜åŒ–å™¨ï¼ˆæ¯”å¦‚AdamWï¼‰
        - epoch: å½“å‰æ˜¯ç¬¬å‡ ä¸ªepoch
        - total_epochs: æ€»å…±æœ‰å¤šå°‘ä¸ªepoch

        è¿”å›ï¼š
        - è¿™ä¸ªepochçš„å¹³å‡æŸå¤±
        """
        # æŠŠæ¨¡å‹è®¾ä¸ºè®­ç»ƒæ¨¡å¼ï¼ä¸€å®šè¦å†™ï¼Œä¸ç„¶è®­ç»ƒä¸äº†ï¼ˆæ¯”å¦‚dropoutå±‚ä¼šç”Ÿæ•ˆï¼‰
        self.model.train()
        total_loss = 0.0  # ç´¯è®¡æŸå¤±

        # å¾ªç¯æ¯ä¸ªbatch
        for batch_idx, batch in enumerate(train_loader):
            # æŠŠæ•°æ®ç§»åˆ°è®¾å¤‡ä¸Šï¼ˆCPU/GPUï¼‰
            input_ids = batch['input_ids'].to(self.device)
            attention_mask = batch['attention_mask'].to(self.device)
            labels = batch['labels'].to(self.device)

            # å‰å‘ä¼ æ’­ï¼šæŠŠæ•°æ®å–‚ç»™æ¨¡å‹ï¼Œå¾—åˆ°è¾“å‡º
            outputs = self.model(
                input_ids=input_ids,
                attention_mask=attention_mask,
                labels=labels  # ä¼ labelsä¼šè‡ªåŠ¨è®¡ç®—loss
            )
            loss = outputs.loss  # å–å‡ºæŸå¤±
            total_loss += loss.item()  # ç´¯åŠ æŸå¤±ï¼ˆè½¬æˆPythonæ•°å­—ï¼‰

            # åå‘ä¼ æ’­ï¼šæ›´æ–°å‚æ•°
            optimizer.zero_grad()  # å…ˆæ¸…ç©ºæ¢¯åº¦ï¼Œä¸ç„¶ä¼šç´¯åŠ 
            loss.backward()  # è®¡ç®—æ¢¯åº¦
            optimizer.step()  # æ›´æ–°å‚æ•°

            # æ¯10ä¸ªbatchæ‰“å°ä¸€æ¬¡è¿›åº¦ï¼Œä¸ç„¶ä¸çŸ¥é“è®­ç»ƒåˆ°å“ªäº†
            if (batch_idx + 1) % 10 == 0:
                avg_loss = total_loss / (batch_idx + 1)  # å¹³å‡æŸå¤±
                print(
                    f'Epoch [{epoch}/{total_epochs}] | Batch [{batch_idx + 1}/{len(train_loader)}] | Loss: {avg_loss:.4f}')

        # è¿”å›è¿™ä¸ªepochçš„å¹³å‡æŸå¤±
        return total_loss / len(train_loader)

    def evaluate(self, val_loader):
        """
        åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°æ¨¡å‹è¡¨ç°

        å‚æ•°ï¼š
        - val_loader: éªŒè¯é›†åŠ è½½å™¨

        è¿”å›ï¼š
        - å¹³å‡éªŒè¯æŸå¤±
        - éªŒè¯å‡†ç¡®ç‡
        """
        # æŠŠæ¨¡å‹è®¾ä¸ºè¯„ä¼°æ¨¡å¼ï¼å…³é—­dropoutç­‰ï¼Œç»“æœæ›´ç¨³å®š
        self.model.eval()
        val_loss = 0.0  # ç´¯è®¡éªŒè¯æŸå¤±
        correct_predictions = 0  # æ­£ç¡®çš„é¢„æµ‹æ•°
        total_predictions = 0  # æ€»é¢„æµ‹æ•°

        # è¯„ä¼°æ—¶ä¸è®¡ç®—æ¢¯åº¦ï¼Œçœå†…å­˜ï¼Œé€Ÿåº¦ä¹Ÿå¿«
        with torch.no_grad():
            for batch in val_loader:
                # æ•°æ®ç§»åˆ°è®¾å¤‡
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)
                labels = batch['labels'].to(self.device)

                # å‰å‘ä¼ æ’­ï¼Œå¾—åˆ°è¾“å‡º
                outputs = self.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    labels=labels
                )

                loss = outputs.loss  # éªŒè¯æŸå¤±
                logits = outputs.logits  # é¢„æµ‹çš„logitsï¼ˆè¿˜æ²¡è½¬æˆæ¦‚ç‡ï¼‰

                val_loss += loss.item()  # ç´¯åŠ éªŒè¯æŸå¤±

                # è®¡ç®—å‡†ç¡®ç‡ï¼šæ‰¾logitsé‡Œæœ€å¤§çš„ç´¢å¼•ä½œä¸ºé¢„æµ‹ç»“æœ
                predictions = torch.argmax(logits, dim=1)
                # ç»Ÿè®¡æ­£ç¡®çš„æ•°é‡ï¼ˆé¢„æµ‹å’Œæ ‡ç­¾ä¸€æ ·çš„ï¼‰
                correct_predictions += torch.sum(predictions == labels).item()
                total_predictions += len(labels)  # æ€»æ•°é‡

        # è®¡ç®—å¹³å‡éªŒè¯æŸå¤±å’Œå‡†ç¡®ç‡
        avg_val_loss = val_loss / len(val_loader)
        accuracy = correct_predictions / total_predictions

        return avg_val_loss, accuracy

    def save_checkpoint(self, model, label_encoder, accuracy, epoch, is_best=False):
        """
        ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹ï¼ˆåŒ…æ‹¬æ¨¡å‹æƒé‡ã€æ ‡ç­¾ç¼–ç å™¨ã€è®­ç»ƒä¿¡æ¯ï¼‰

        å‚æ•°ï¼š
        - model: è¦ä¿å­˜çš„æ¨¡å‹
        - label_encoder: æ ‡ç­¾ç¼–ç å™¨ï¼ˆé¢„æµ‹æ—¶è¦ç”¨ï¼‰
        - accuracy: å½“å‰çš„å‡†ç¡®ç‡
        - epoch: å½“å‰epoch
        - is_best: æ˜¯å¦æ˜¯ç›®å‰æœ€å¥½çš„æ¨¡å‹

        è¿”å›ï¼š
        - ä¿å­˜çš„è·¯å¾„
        """
        # ç¡®å®šä¿å­˜ç›®å½•ï¼šæœ€å¥½çš„æ¨¡å‹æ”¾best_modelï¼Œå…¶ä»–æŒ‰epochå·å‘½å
        if is_best:
            model_dir = os.path.join(self.save_path, "best_model")
        else:
            model_dir = os.path.join(self.save_path, f"epoch_{epoch}")

        # åˆ›å»ºç›®å½•
        os.makedirs(model_dir, exist_ok=True)

        # ä¿å­˜æ¨¡å‹æƒé‡å’Œé…ç½®
        model.save_pretrained(model_dir)
        # ä¿å­˜æ ‡ç­¾ç¼–ç å™¨ï¼ˆç”¨joblibï¼‰
        joblib.dump(label_encoder, os.path.join(model_dir, "label_encoder.pkl"))

        # ä¿å­˜ä¸€äº›è®­ç»ƒä¿¡æ¯ï¼Œæ–¹ä¾¿ä»¥åæŸ¥çœ‹
        info_file = os.path.join(model_dir, "training_info.txt")
        with open(info_file, 'w', encoding='utf-8') as f:
            f.write(f"Epoch: {epoch}\n")  # ç¬¬å‡ ä¸ªepoch
            f.write(f"Accuracy: {accuracy:.4f}\n")  # å‡†ç¡®ç‡
            f.write(f"Classes: {list(label_encoder.classes_)}\n")  # æœ‰å“ªäº›ç±»åˆ«
            f.write(f"Best Model: {is_best}\n")  # æ˜¯å¦æ˜¯æœ€ä½³æ¨¡å‹

        print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_dir}")
        return model_dir  # è¿”å›ä¿å­˜è·¯å¾„


def train_bert_model(
        model,
        train_loader,
        val_loader,
        label_encoder,
        device,
        save_path,
        learning_rate=3e-5,
        epochs=3
):
    """
    è®­ç»ƒBERTæ¨¡å‹çš„ä¸»å‡½æ•°ï¼ŒæŠŠä¸Šé¢çš„Trainerä¸²èµ·æ¥ç”¨

    å‚æ•°ï¼š
    - model: BERTæ¨¡å‹
    - train_loader: è®­ç»ƒé›†åŠ è½½å™¨
    - val_loader: éªŒè¯é›†åŠ è½½å™¨
    - label_encoder: æ ‡ç­¾ç¼–ç å™¨
    - device: è®¾å¤‡
    - save_path: ä¿å­˜è·¯å¾„
    - learning_rate: å­¦ä¹ ç‡ï¼ˆä¸€èˆ¬3e-5æ¯”è¾ƒåˆé€‚ï¼Œåˆ«æ”¹å¤ªå¤§ï¼‰
    - epochs: è®­ç»ƒè½®æ•°ï¼ˆå¤ªå°‘æ¬ æ‹Ÿåˆï¼Œå¤ªå¤šè¿‡æ‹Ÿåˆï¼Œå…ˆè¯•è¯•3è½®ï¼‰

    è¿”å›ï¼š
    - æœ€ä½³æ¨¡å‹çš„ä¿å­˜è·¯å¾„
    """
    # åˆå§‹åŒ–è®­ç»ƒå™¨
    trainer = BERTTrainer(model, device, save_path)
    # ä¼˜åŒ–å™¨ç”¨AdamWï¼ŒBERTä¸€èˆ¬éƒ½ç”¨è¿™ä¸ª
    optimizer = optim.AdamW(model.parameters(), lr=learning_rate)

    best_accuracy = 0.0  # è®°å½•æœ€å¥½çš„å‡†ç¡®ç‡ï¼Œä¸€å¼€å§‹æ˜¯0
    best_model_path = None  # æœ€å¥½çš„æ¨¡å‹è·¯å¾„

    print("å¼€å§‹è®­ç»ƒBERTæ¨¡å‹...")

    # å¾ªç¯æ¯ä¸ªepoch
    for epoch in range(1, epochs + 1):
        print(f"\n{'=' * 60}")
        print(f'Epoch {epoch}/{epochs}')  # æ˜¾ç¤ºå½“å‰æ˜¯ç¬¬å‡ ä¸ªepoch
        print(f"{'=' * 60}")

        # è®­ç»ƒé˜¶æ®µï¼šè¿”å›è¿™ä¸ªepochçš„å¹³å‡æŸå¤±
        train_loss = trainer.train_epoch(train_loader, optimizer, epoch, epochs)
        print(f'è®­ç»ƒæŸå¤±: {train_loss:.4f}')  # æ‰“å°è®­ç»ƒæŸå¤±

        # éªŒè¯é˜¶æ®µï¼šè¿”å›å¹³å‡æŸå¤±å’Œå‡†ç¡®ç‡
        val_loss, accuracy = trainer.evaluate(val_loader)
        print(f'éªŒè¯æŸå¤±: {val_loss:.4f} | éªŒè¯å‡†ç¡®ç‡: {accuracy:.4f}')  # æ‰“å°éªŒè¯ç»“æœ

        # ä¿å­˜å½“å‰epochçš„æ¨¡å‹ï¼ˆä¸æ˜¯æœ€å¥½çš„ä¹Ÿä¿å­˜ï¼Œä¸‡ä¸€åé¢è¦ç”¨ï¼‰
        trainer.save_checkpoint(model, label_encoder, accuracy, epoch)

        # å¦‚æœå½“å‰å‡†ç¡®ç‡æ¯”ä¹‹å‰æœ€å¥½çš„é«˜ï¼Œå°±æ›´æ–°æœ€ä½³æ¨¡å‹
        if accuracy > best_accuracy:
            best_accuracy = accuracy
            # ä¿å­˜ä¸ºæœ€ä½³æ¨¡å‹
            best_model_path = trainer.save_checkpoint(
                model, label_encoder, accuracy, epoch, is_best=True
            )
            print(f"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹ï¼å‡†ç¡®ç‡: {accuracy:.4f}")  # åº†ç¥ä¸€ä¸‹

    print(f"\nè®­ç»ƒå®Œæˆï¼")
    print(f"æœ€ä½³å‡†ç¡®ç‡: {best_accuracy:.4f}")
    print(f"æœ€ä½³æ¨¡å‹è·¯å¾„: {best_model_path}")  # æœ€åæ‰“å°æœ€ä½³æ¨¡å‹è·¯å¾„

    return best_model_path  # è¿”å›æœ€ä½³æ¨¡å‹è·¯å¾„ï¼Œä¾›é¢„æµ‹ç”¨