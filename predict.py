import torch
import joblib
import os
import json
from transformers import BertTokenizerFast, BertForSequenceClassification
from datetime import datetime

# åˆ›å»ºå¿…è¦çš„æ–‡ä»¶å¤¹
def create_necessary_dirs():
    """åˆ›å»ºè¾“å…¥è¾“å‡ºæ–‡ä»¶å¤¹ï¼ˆå¦‚æžœä¸å­˜åœ¨ï¼‰"""
    if not os.path.exists("./sort_input"):
        os.makedirs("./sort_input")
        print("âœ… å·²åˆ›å»ºè¾“å…¥æ–‡ä»¶å¤¹: ./sort_input")
    if not os.path.exists("./sort_output"):
        os.makedirs("./sort_output")
        print("âœ… å·²åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹: ./sort_output")

# è¯»å–é…ç½®æ–‡ä»¶
with open('config.json', 'r', encoding='utf-8') as file:
    config = json.load(file)
best_model_path = config["BEST_MODEL_PATH"]

# ===== æ ‡æ³¨ä¸Žæ—¥å¿—é…ç½® =====
ANNOTATIONS_DIR = "./sort_output"
ANNOTATIONS_CSV = os.path.join(ANNOTATIONS_DIR, "annotations.csv")
ANNOTATIONS_JSONL = os.path.join(ANNOTATIONS_DIR, "annotations.jsonl")
ENABLE_HUMAN_LABELING = True
ALLOW_NEW_LABELS = True
STRICT_LABEL_CHECK = False

def _ensure_annotations_dir():
    os.makedirs(ANNOTATIONS_DIR, exist_ok=True)

def _append_csv(row: dict):
    import csv
    _ensure_annotations_dir()
    headers = ["source", "filename", "pred_label", "confidence", "human_labels", "new_labels", "invalid_labels", "timestamp"]
    write_header = not os.path.exists(ANNOTATIONS_CSV)
    with open(ANNOTATIONS_CSV, "a", encoding="utf-8", newline="") as f:
        w = csv.DictWriter(f, fieldnames=headers)
        if write_header:
            w.writeheader()
        w.writerow(row)

def _append_jsonl(obj: dict):
    _ensure_annotations_dir()
    with open(ANNOTATIONS_JSONL, "a", encoding="utf-8") as f:
        f.write(json.dumps(obj, ensure_ascii=False) + "\n")

def _parse_human_labels(input_str: str, known_classes: list, allow_new=True, strict=False):
    tokens = [x.strip() for x in (input_str or "").split(",") if x.strip()]
    if not tokens:
        return [], [], []
    known_set = set(known_classes or [])
    valid, new, invalid = [], [], []
    for t in tokens:
        if t in known_set:
            valid.append(t)
        else:
            if allow_new and not strict:
                new.append(t)
            else:
                invalid.append(t)
    return valid, new, invalid

def sanitize_text(raw_text, *, max_chars=500, min_chars=10):
    """æ ¡éªŒä¸Žæˆªæ–­æ–‡æœ¬"""
    if raw_text is None:
        raise ValueError("ç©ºæ–‡æœ¬")
    text = str(raw_text).replace("\r\n", "\n").strip()
    if not text:
        raise ValueError("ç©ºæ–‡æœ¬")
    if "\x00" in text:
        raise ValueError("ç–‘ä¼¼äºŒè¿›åˆ¶æ–‡ä»¶ï¼ˆåŒ…å«NULï¼‰")
    bad_count = text.count("ï¿½")
    if bad_count and (bad_count / max(1, len(text)) > 0.01):
        print("âš ï¸ æ–‡æœ¬åŒ…å«ä¹±ç å­—ç¬¦ï¼Œè¯·æ£€æŸ¥ç¼–ç ï¼ˆUTF-8 æŽ¨èï¼‰")
    truncated = False
    if len(text) > max_chars:
        text = text[:max_chars]
        truncated = True
    if len(text) < min_chars:
        raise ValueError(f"æ–‡æœ¬è¿‡çŸ­ï¼ˆå°‘äºŽ {min_chars} å­—ï¼‰")
    return text, truncated


class TextClassifier:
    def __init__(self, model_path, device=None):
        """åˆå§‹åŒ–æ¨¡åž‹"""
        self.device = torch.device("cuda" if (torch.cuda.is_available() and device != "cpu") else "cpu")
        print(f"ðŸ”§ ä½¿ç”¨è®¾å¤‡: {self.device}")

        self.model_dir = model_path
        self.base_model_name = config["MODEL_PATH"]
        self.max_length = config["MAX_LENGTH"]
        self.num_labels = config["NUM_CLASSES"]

        try:
            self.tokenizer = BertTokenizerFast.from_pretrained(self.base_model_name)
            self.model = BertForSequenceClassification.from_pretrained(
                self.base_model_name, num_labels=self.num_labels)
            wt_bin = os.path.join(self.model_dir, "pytorch_model.bin")
            wt_safe = os.path.join(self.model_dir, "model.safetensors")
            if os.path.exists(wt_safe):
                from safetensors.torch import load_file
                self.model.load_state_dict(load_file(wt_safe))
            elif os.path.exists(wt_bin):
                self.model.load_state_dict(torch.load(wt_bin, map_location=self.device))
            else:
                raise FileNotFoundError("æœªæ‰¾åˆ°æ¨¡åž‹æƒé‡æ–‡ä»¶ï¼")
            self.model.to(self.device)
            self.model.eval()
            self.label_encoder = joblib.load(os.path.join(self.model_dir, "label_encoder.pkl"))
            print(f"âœ… æ¨¡åž‹åŠ è½½æˆåŠŸï¼ˆè·¯å¾„ï¼š{self.model_dir}ï¼‰")
        except Exception as e:
            raise RuntimeError(f"æ¨¡åž‹åˆå§‹åŒ–å¤±è´¥ï¼š{e}")

    def predict(self, text):
        """é¢„æµ‹å•æ–‡æœ¬ç±»åˆ«"""
        inputs = self.tokenizer(
            text, max_length=self.max_length, truncation=True, padding=True, return_tensors="pt")
        inputs = {k: v.to(self.device) for k, v in inputs.items()}
        with torch.no_grad():
            outputs = self.model(**inputs)
            probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
            conf, idx = torch.max(probs, dim=1)
            label = self.label_encoder.inverse_transform([idx.cpu().item()])[0]
            return label, conf.cpu().item()

    def process_file(self, input_path):
        """æ–‡ä»¶é¢„æµ‹+äººå·¥æ ‡æ³¨"""
        try:
            with open(input_path, 'r', encoding='utf-8', errors='ignore') as f:
                raw_text = f.read()
            text, truncated = sanitize_text(raw_text)
            pred_label, confidence = self.predict(text)

            # è¾“å‡ºç»“æžœæ–‡ä»¶
            filename = os.path.basename(input_path)
            output_path = os.path.join("./sort_output", f"{os.path.splitext(filename)[0]}_result.txt")
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write("æ–‡æœ¬åˆ†ç±»ç»“æžœ\n=============\n")
                f.write(f"æ–‡ä»¶åç§°: {filename}\né¢„æµ‹ç±»åˆ«: {pred_label}\nç½®ä¿¡åº¦: {confidence:.2%}\n")
                if truncated:
                    f.write("(æç¤ºï¼šè¾“å…¥å·²æˆªæ–­è‡³500å­—)\n")
                f.write("\næ–‡æœ¬é¢„è§ˆ:\n" + text)

            # äººå·¥æ ‡æ³¨
            if os.environ.get('NO_LABEL_PROMPT') != '1' and ENABLE_HUMAN_LABELING:
                classes = list(getattr(self.label_encoder, 'classes_', []))
                if classes:
                    print("å¯é€‰ç±»åˆ«: " + ", ".join(classes))
                gt = input("è¯·è¾“å…¥å®žé™…ç±»åˆ«ï¼ˆå¯å¤šä¸ªï¼Œè‹±æ–‡é€—å·åˆ†éš”ï¼Œå›žè½¦è·³è¿‡ï¼‰: ").strip()
                valid, new, invalid = _parse_human_labels(gt, classes, ALLOW_NEW_LABELS, STRICT_LABEL_CHECK)
                if invalid:
                    print(f"âš ï¸ å¿½ç•¥æœªçŸ¥æ ‡ç­¾: {', '.join(invalid)}")
                if new:
                    print(f"â„¹ï¸ å·²è®°å½•æ–°å¢žæ ‡ç­¾: {', '.join(new)}")

                ts = datetime.now().strftime("%Y%m%d_%H%M%S")
                row = dict(
                    source='file', filename=filename, pred_label=pred_label,
                    confidence=f"{confidence:.4f}",
                    human_labels="|".join(valid), new_labels="|".join(new),
                    invalid_labels="|".join(invalid), timestamp=ts
                )
                _append_csv(row)
                _append_jsonl({**row, "text": text})

            print(f"ðŸ“„ å·²å¤„ç† {filename} â†’ {os.path.basename(output_path)}")
            return output_path
        except Exception as e:
            print(f"âŒ æ–‡ä»¶ {os.path.basename(input_path)} å‡ºé”™: {e}")


def process_terminal_input(classifier):
    """ç»ˆç«¯è¾“å…¥æ¨¡å¼"""
    print("\nðŸ“ è¯·è¾“å…¥è¦åˆ†ç±»çš„æ–‡æœ¬ï¼ˆç©ºè¡Œç»“æŸï¼‰")
    print("=" * 60)
    lines = []
    while True:
        line = input()
        if not line:
            break
        lines.append(line)
    if not lines:
        print("â• æœªè¾“å…¥ä»»ä½•æ–‡æœ¬")
        return

    text, truncated = sanitize_text("\n".join(lines))
    label, conf = classifier.predict(text)
    print("\n" + "=" * 60)
    print(f"é¢„æµ‹ç±»åˆ«: {label}\nç½®ä¿¡åº¦: {conf:.2%}")
    if truncated:
        print("(æç¤ºï¼šè¾“å…¥å·²æˆªæ–­è‡³500å­—)")
    print("=" * 60)

    # äººå·¥æ ‡æ³¨åŒæ ·é€‚ç”¨
    if ENABLE_HUMAN_LABELING and os.environ.get('NO_LABEL_PROMPT') != '1':
        classes = list(getattr(classifier.label_encoder, 'classes_', []))
        if classes:
            print("å¯é€‰ç±»åˆ«: " + ", ".join(classes))
        gt = input("è¯·è¾“å…¥å®žé™…ç±»åˆ«ï¼ˆå¯å¤šä¸ªï¼Œè‹±æ–‡é€—å·åˆ†éš”ï¼Œå›žè½¦è·³è¿‡ï¼‰: ").strip()
        valid, new, invalid = _parse_human_labels(gt, classes, ALLOW_NEW_LABELS, STRICT_LABEL_CHECK)
        ts = datetime.now().strftime("%Y%m%d_%H%M%S")
        row = dict(
            source='terminal', filename='-', pred_label=label,
            confidence=f"{conf:.4f}",
            human_labels="|".join(valid), new_labels="|".join(new),
            invalid_labels="|".join(invalid), timestamp=ts
        )
        _append_csv(row)
        _append_jsonl({**row, "text": text})
        print("âœ… æ ‡æ³¨å·²ä¿å­˜")

def print_welcome_message():
    print("=" * 60)
    print("æ¬¢è¿Žä½¿ç”¨ BERT æ–‡æœ¬åˆ†ç±»å·¥å…·")
    print("=" * 60)
    print("1 - æ–‡ä»¶è¾“å…¥ï¼ˆ./sort_inputï¼‰")
    print("2 - ç»ˆç«¯è¾“å…¥")
    print("3 - åˆ‡æ¢æ˜¯å¦å…è®¸æ–°å¢žç±»åˆ«")
    print("4 - åˆ‡æ¢ä¸¥æ ¼æ¨¡å¼")
    print("0 - é€€å‡º")
    print("=" * 60)

def main():
    create_necessary_dirs()
    print_welcome_message()
    classifier = TextClassifier(best_model_path)

    while True:
        choice = input("è¯·è¾“å…¥é€‰é¡¹ (0-4): ").strip()
        if choice == '1':
            for f in os.listdir('./sort_input'):
                if f.endswith('.txt'):
                    classifier.process_file(os.path.join('./sort_input', f))
        elif choice == '2':
            process_terminal_input(classifier)
        elif choice == '3':
            globals()['ALLOW_NEW_LABELS'] = not ALLOW_NEW_LABELS
            print(f"å·²åˆ‡æ¢: å…è®¸æ–°å¢žç±»åˆ« = {'æ˜¯' if ALLOW_NEW_LABELS else 'å¦'}")
        elif choice == '4':
            globals()['STRICT_LABEL_CHECK'] = not STRICT_LABEL_CHECK
            print(f"å·²åˆ‡æ¢: ä¸¥æ ¼æ¨¡å¼ = {'æ˜¯' if STRICT_LABEL_CHECK else 'å¦'}")
        elif choice == '0':
            print("ðŸ‘‹ å·²é€€å‡ºç¨‹åºã€‚")
            break
        else:
            print("æ— æ•ˆé€‰é¡¹ï¼Œè¯·é‡æ–°è¾“å…¥ã€‚")

if __name__ == "__main__":
    main()
